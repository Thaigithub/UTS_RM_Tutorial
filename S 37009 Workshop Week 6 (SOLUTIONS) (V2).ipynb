{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b43d3e",
   "metadata": {},
   "source": [
    "# 37009 Workshop Week 6: Analytical VaR\n",
    "\n",
    "---\n",
    "\n",
    "## Change Log\n",
    "\n",
    "1. (2023-09-14) The cash flow mapping implementation has been corrected\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Today, 21 December 2021, we hold a portfolio consisting of bonds and physical assets. The specification of each component is as follows:\n",
    "1. A coupon bond with face value of \\$2 million paying semi-annual coupons at a rate of 4\\% per annum which matures on 21 December 2022.\n",
    "2. A coupon bond with face value of \\$2 million paying semi-annual coupons at a rate of 2\\% per annum which matures on 21 December 2023.\n",
    "3. A short position on 1000 shares of the ASX 200\n",
    "4. A long position on 2500 shares of S\\&P 500 (*Note: Prices provided in the data set are prices of the S\\&P500 in USD*)\n",
    "5. A long position on US\\$4 million\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Given the time series of financial data:\n",
    "\n",
    "1. Determine the marked-to-market value of the portfolio today.\n",
    "2. Determine the risk factors driving the portfolio. Calculate the daily continuously compounded returns (log-returns) of the risk factors using the data provided.\n",
    "3. Compute the **undiversified** one-day and 10-day 90\\% value-at-risk for the entire portfolio using the delta-normal approach (*Hint: You will need to determine the VaR for each portfolio component/instrument*)\n",
    "4. Compute the **diversified** one-day and 10-day 90\\% value-at-risk for the entire portfolio using the delta-normal approach (*Hint: You will need to compute the variance-covariance matrix of the returns of the risk factors*)\n",
    "\n",
    "**NOTE:** We will use the log-returns of the risk factors to calculate the VaR, since using log-returns allow us to perform square-root-of-time scaling to compute the 10-day VaR from the 1-day VaR. The time scaling can be used on the VaR computed from percentage returns, although this adds another layer of approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e12b96",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This section loads the required libraries and the cash flow mapping function which may be useful in mapping the cash flows of the bonds in the portfolio. We also load the historical data we are provided.\n",
    "\n",
    "**NOTE:** For simplicity, zero rate, index, and exchange rate data have been provided for the same trading days. If you have acquired data from different sources, this may not be the case. As such, you will need to merge multiple data sets using some joining rule; for example, keeping the data for the dates that are common to your data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2122b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Cash flow mapping function\n",
    "def CFmap2V(maturity, left_maturity, right_maturity, map_type = \"Duration\", \n",
    "            left_vol = None, right_vol = None, left_right_cor = None):\n",
    "    \n",
    "    # Assumption: Cash flow at non-standard maturity is mapped to two vertices, one on the left and one on the right.\n",
    "    # Imposes PV-invariance; may impose duration-invariance or volatility-invariance, but not both.\n",
    "    \n",
    "    # Output: alpha parameter to allocate a PV of 1 to the left vertex T1. \n",
    "    # For the volatility mapping, the function returns two values of alpha. \n",
    "    # We often will choose the alpha that is between 0 and 1.\n",
    "    \n",
    "    # Use mathematical notation for function inputs\n",
    "    T = maturity\n",
    "    T1 = left_maturity\n",
    "    T2 = right_maturity\n",
    "    \n",
    "    if map_type == \"Duration\":\n",
    "        alpha = (T2 - T) / (T2 - T1)\n",
    "        return alpha\n",
    "    \n",
    "    if map_type == \"Volatility\":\n",
    "        \n",
    "        if (left_vol == None or right_vol == None or left_right_cor == None):\n",
    "            print(\"Please provide non-empty volatility-invariant mapping inputs\")\n",
    "        \n",
    "        else:\n",
    "            # Extract volatility-invariant CF map inputs\n",
    "            sigma1 = left_vol\n",
    "            sigma2 = right_vol\n",
    "            rho = left_right_cor\n",
    "            \n",
    "            # Linearly interpolate the variance at the non-standard maturity\n",
    "            sq_sigma_interp = np.interp(x = T, xp = np.array([T1, T2]), fp = np.array([sigma1 ** 2, sigma2 ** 2]))\n",
    "            \n",
    "            # Coefficients of quadratic equation for alpha\n",
    "            a_coeff = sigma1 ** 2 - 2 * rho * sigma1 * sigma2 + sigma2 ** 2\n",
    "            b_coeff = 2 * rho * sigma1 * sigma2 - 2 * sigma2 ** 2\n",
    "            c_coeff = sigma2 ** 2 - sq_sigma_interp\n",
    "            \n",
    "            # Solve quadratic equation for alpha\n",
    "            alpha1 = (-b_coeff + np.sqrt(b_coeff ** 2 - 4 * a_coeff * c_coeff)) / (2 * a_coeff)\n",
    "            alpha2 = (-b_coeff - np.sqrt(b_coeff ** 2 - 4 * a_coeff * c_coeff)) / (2 * a_coeff)\n",
    "            alpha = np.array([alpha1, alpha2])\n",
    "            \n",
    "            return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5b0a6",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95ca214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data\n",
    "fin_data = pd.read_csv('C:/WorkshopWeek4Data-1.csv', index_col = 0)\n",
    "\n",
    "# Write the exchange rates as XX AUD/USD\n",
    "fin_data['FXrate'] = 1 / fin_data['AUDUSD']\n",
    "\n",
    "# # Compute historical prices of S&P500 in AUD\n",
    "# fin_data['SPXCompAUD'] = fin_data['SPXComp'] * fin_data['FXrate']a\n",
    "# fin_data.tail()\n",
    "\n",
    "# Compute zero coupon bond prices\n",
    "maturities = np.array([1/360, 1/12, 2/12, 3/12, 6/12, 9/12, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "zcb = np.exp(-fin_data.iloc[:, 3:(3 + len(maturities))] * maturities / 100)\n",
    "\n",
    "# Extract data for mark-to-market valuation (observations on 21 December 2021)\n",
    "fin_data_today = fin_data.loc['2021-12-21']\n",
    "zcb_today = zcb.loc['2021-12-21']\n",
    "\n",
    "# Compute log-returns for all market variables\n",
    "fin_data_logret = np.log(fin_data)\n",
    "fin_data_logret = fin_data_logret.diff().tail(-1)\n",
    "zcb_logret = np.log(zcb)\n",
    "zcb_logret = zcb_logret.diff().tail(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3318622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASX200</th>\n",
       "      <th>SPXComp</th>\n",
       "      <th>AUDUSD</th>\n",
       "      <th>AU00Y00</th>\n",
       "      <th>AU00Y01</th>\n",
       "      <th>AU00Y02</th>\n",
       "      <th>AU00Y03</th>\n",
       "      <th>AU00Y06</th>\n",
       "      <th>AU00Y09</th>\n",
       "      <th>AU01Y00</th>\n",
       "      <th>AU02Y00</th>\n",
       "      <th>AU03Y00</th>\n",
       "      <th>AU04Y00</th>\n",
       "      <th>AU05Y00</th>\n",
       "      <th>AU06Y00</th>\n",
       "      <th>AU07Y00</th>\n",
       "      <th>AU08Y00</th>\n",
       "      <th>AU09Y00</th>\n",
       "      <th>FXrate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>0.004594</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>-0.010816</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.010533</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.011777</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.010816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>0.002645</td>\n",
       "      <td>-0.018447</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.001364</td>\n",
       "      <td>-0.009495</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>-0.009894</td>\n",
       "      <td>-0.014479</td>\n",
       "      <td>-0.015795</td>\n",
       "      <td>-0.017115</td>\n",
       "      <td>-0.018466</td>\n",
       "      <td>-0.018859</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>-0.015817</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>-0.003221</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-0.003678</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>-0.003811</td>\n",
       "      <td>-0.002125</td>\n",
       "      <td>-0.009586</td>\n",
       "      <td>-0.015978</td>\n",
       "      <td>-0.018991</td>\n",
       "      <td>-0.023430</td>\n",
       "      <td>-0.029457</td>\n",
       "      <td>-0.034281</td>\n",
       "      <td>-0.035122</td>\n",
       "      <td>-0.039023</td>\n",
       "      <td>0.003221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>-0.002088</td>\n",
       "      <td>0.011563</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>-0.002231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>-0.003611</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.019625</td>\n",
       "      <td>0.019157</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.016902</td>\n",
       "      <td>0.016417</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>-0.005679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ASX200   SPXComp    AUDUSD   AU00Y00   AU00Y01   AU00Y02  \\\n",
       "Date                                                                     \n",
       "2018-11-28  0.004594 -0.000340 -0.010816  0.007701  0.007701  0.003756   \n",
       "2018-11-29  0.002645 -0.018447 -0.000865 -0.000075 -0.000075 -0.000074   \n",
       "2018-11-30 -0.015817 -0.008933 -0.003221  0.000037  0.000037  0.001910   \n",
       "2018-12-01 -0.002088  0.011563  0.002231  0.000037  0.000037 -0.001836   \n",
       "2018-12-04  0.005187  0.017730  0.005679  0.000000  0.000000  0.000000   \n",
       "\n",
       "             AU00Y03   AU00Y06   AU00Y09   AU01Y00   AU02Y00   AU03Y00  \\\n",
       "Date                                                                     \n",
       "2018-11-28  0.003714  0.000035  0.004273  0.002274  0.010533  0.012632   \n",
       "2018-11-29  0.000036  0.000000 -0.000269 -0.001364 -0.009495 -0.006255   \n",
       "2018-11-30 -0.003678  0.001801 -0.003811 -0.002125 -0.009586 -0.015978   \n",
       "2018-12-01 -0.003691 -0.001801  0.001696  0.001367  0.006387  0.007480   \n",
       "2018-12-04  0.001811 -0.003611  0.006792  0.005185  0.022100  0.021988   \n",
       "\n",
       "             AU04Y00   AU05Y00   AU06Y00   AU07Y00   AU08Y00   AU09Y00  \\\n",
       "Date                                                                     \n",
       "2018-11-28  0.011777  0.011330  0.008909  0.005752  0.002781  0.001845   \n",
       "2018-11-29 -0.009894 -0.014479 -0.015795 -0.017115 -0.018466 -0.018859   \n",
       "2018-11-30 -0.018991 -0.023430 -0.029457 -0.034281 -0.035122 -0.039023   \n",
       "2018-12-01  0.004918  0.001746  0.005706  0.010458  0.012061  0.015567   \n",
       "2018-12-04  0.019625  0.019157  0.017499  0.016902  0.016417  0.014152   \n",
       "\n",
       "              FXrate  \n",
       "Date                  \n",
       "2018-11-28  0.010816  \n",
       "2018-11-29  0.000865  \n",
       "2018-11-30  0.003221  \n",
       "2018-12-01 -0.002231  \n",
       "2018-12-04 -0.005679  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_data_logret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a10e0",
   "metadata": {},
   "source": [
    "## Mark-to-Market Valuation\n",
    "\n",
    "### Physical Assets\n",
    "\n",
    "The valuation of the phyiscal asset holdings is straightforward; simply multiply the number of units by the spot price today. It will also be helpful to express the valuation mathematically to facilitate the risk factor mapping for the entire portfolio. We have\n",
    "\n",
    "$$\\begin{align*}\n",
    "V_{\\text{ASX}}(t) & = -1,000 \\cdot S_{\\text{ASX}}(t)\\\\\n",
    "V_{\\text{SPX}}(t) & = 2,500 \\cdot S_{\\text{SPX}}(t) \\cdot S_{\\text{FX}}(t)\\\\\n",
    "V_{\\text{USD}}(t) & = 4,000,000 \\cdot S_{\\text{FX}}(t)\n",
    "\\end{align*}$$\n",
    "\n",
    "where the $S_{\\text{FX}}(t)$ is the spot exchange rate in AUD per USD and the other variables are indicated by the subscript.\n",
    "\n",
    "**EXERCISE:** How will the analysis differ if we convert the price of the SPX to AUD and use the AUD price as a risk factor instead of both the USD price of the SPX and the exchange rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f27998a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6060358.0 8713873.019994806 5193456.245131135\n"
     ]
    }
   ],
   "source": [
    "# Mark-to-market valuation for physical assets\n",
    "\n",
    "# Number of units (negative for short position)\n",
    "N_ASX = -1000\n",
    "N_SPX = 2500\n",
    "N_USD = 4000000\n",
    "\n",
    "# Valuation\n",
    "V_ASX = N_ASX * fin_data_today['ASX200']\n",
    "V_SPX = N_SPX * fin_data_today['SPXComp'] * fin_data_today['FXrate']\n",
    "V_USD = N_USD * fin_data_today['FXrate']\n",
    "\n",
    "print(V_ASX, V_SPX, V_USD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b4c80",
   "metadata": {},
   "source": [
    "### Bonds\n",
    "\n",
    "The table below summarizes the cash flows from the bonds:\n",
    "\n",
    "| Maturity | 6 months | 1 year | 1.5 years | 2 years |\n",
    "|:-|---|---|---|---|\n",
    "| Bond 1 | 40,000 | 2,040,000 | | |\n",
    "| Bond 2 | 20,000 | 20,000 | 20,000 | 2,020,000|\n",
    "\n",
    "We do not have data on the zero rates with a 1.5-year maturity, so for risk management purposes, the cash flow to be received in 1.5 years must be mapped to the 1-year and 2-year vertices. ~~There are two ways to go about this:~~\n",
    "\n",
    "1. ~~For valuation purposes, interpolate the 1.5-year zero rate from the 1-year and 2-year zero rates~~\n",
    "2. ~~Perform a present-value invariant cash flow mapping to map the 1.5-year CF to the 1-year and 2-year standard vertices.~~\n",
    "\n",
    "~~As we will eventually need to map the 1.5-year cash flow of Bond 2 anyway for risk management purposes, we can perform this step now in the valuation as well. Since PV-invariance is imposed, we will get the same PV and, hence, the same marked-to-market value of the bond.~~\n",
    "\n",
    "#### Cash Flow Mapping\n",
    "\n",
    "**A PV- and volatility-invariant cash flow map will be used.** To this end, we need to estimate the standard deviations and correlation of the log-returns of zero coupon bonds with 1-year and 2-year maturities.\n",
    "\n",
    "We also need to interpolate the 1.5-year zero coupon bond price (to be used as a discount factor), since the cash flow mapping is performed on the present values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071159bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3436885659581919 6802.928186028253 13264.358719107897\n"
     ]
    }
   ],
   "source": [
    "# Interpolate the 1.5-year ZCB price\n",
    "zcb_18month = np.interp(x = 1.5, xp = np.array([1, 2]), fp = np.array([zcb_today['AU01Y00'], zcb_today['AU02Y00']]))\n",
    "\n",
    "# Compute standard deviation and correlation of log-returns of the 1-year and 2-year zero coupon bonds\n",
    "zcb_1y_std = np.std(zcb_logret['AU01Y00'])\n",
    "zcb_2y_std = np.std(zcb_logret['AU02Y00'])\n",
    "zcb_1y2y_corr = np.corrcoef(zcb_logret['AU01Y00'], zcb_logret['AU02Y00'])[1,0]\n",
    "\n",
    "# Compute cash flow mapping coefficient\n",
    "cf_map_coef = CFmap2V(1.5, 1, 2, \"Volatility\", zcb_1y_std, zcb_2y_std, zcb_1y2y_corr)\n",
    "cf_map_coef = cf_map_coef[np.where((cf_map_coef >= 0) & (cf_map_coef <= 1))[0]].item()     # .item() so that it returns a float rather than a singleton array\n",
    "\n",
    "# Calculate amounts to map to standard vertices\n",
    "cf_map_to_1y = cf_map_coef * 20000 * zcb_18month / zcb_today['AU01Y00']\n",
    "cf_map_to_2y = (1 - cf_map_coef) * 20000 * zcb_18month / zcb_today['AU02Y00']\n",
    "\n",
    "print(cf_map_coef, cf_map_to_1y, cf_map_to_2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce637f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3436885659581919"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_map_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ac99a",
   "metadata": {},
   "source": [
    "The mapped cash flows are as follows (manually entered):\n",
    "\n",
    "| Maturity | 6 months | 1 year | 1.5 years | 2 years |\n",
    "|:-|---|---|---|---|\n",
    "| Bond 1 | 40,000.00 | 2,040,000.00 | | |\n",
    "| Bond 2 | 20,000.00 | 20,000.00 | 20,000.00 | 2,020,000.00\n",
    "| Bond 2 (Mapped) | 20,000.00 | 26,802.93 | | 2,033,264.36|\n",
    "\n",
    "#### Mark-to-Marked Valuation\n",
    "\n",
    "We are now ready to compute the marked-to-market values of the two bonds, which are given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "V_{B_1}(t) & = 40,000 \\cdot P(0,0.5) + 2,040,000 \\cdot P(0,1)\\\\\n",
    "V_{B_2}(t) & = 20,000 \\cdot P(0,0.5) + 26,802.93 \\cdot P(0,1) + 2,033,264.36 \\cdot P(0,2)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c7c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039883.2875202913 1998664.0060008303\n"
     ]
    }
   ],
   "source": [
    "# Bond parameters\n",
    "B1_fv = 2000000\n",
    "B1_coup_rate = 0.04\n",
    "B1_coup = B1_fv * B1_coup_rate / 2\n",
    "\n",
    "B2_fv = 2000000\n",
    "B2_coup_rate = 0.02\n",
    "B2_coup = B2_fv * B2_coup_rate / 2\n",
    "\n",
    "# Valuation (manually done since there are only a few CFs)\n",
    "V_B1 = B1_coup * zcb_today['AU00Y06'] + (B1_fv + B1_coup) * zcb_today['AU01Y00']\n",
    "V_B2 = B2_coup * zcb_today['AU00Y06'] + (B2_coup + cf_map_to_1y) * zcb_today['AU01Y00'] \\\n",
    "        + (B2_fv + B2_coup + cf_map_to_2y) * zcb_today['AU02Y00']\n",
    "\n",
    "print(V_B1, V_B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d09224",
   "metadata": {},
   "source": [
    "The total value of the portfolio is the sum of the all the marked-to-market values computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75061643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11885518.558647063"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_portfolio = V_ASX + V_SPX + V_USD + V_B1 + V_B2\n",
    "V_portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ff1d1",
   "metadata": {},
   "source": [
    "## Risk Factor Mapping\n",
    "\n",
    "From the mark-to-market valuation step, we conclude that the risk factors for the portfolio are the ASX spot price, the S\\&P 500 spot price (in USD), the spot exchange rate in AUD per USD, and the prices of zero coupon bonds with a 6-month, a 1-year, and a 2-year maturity.\n",
    "\n",
    "With respect to these risk factors, a delta approximation for the change in the value of each component of the portfolio is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\Delta V_{\\text{ASX}} & = N_{\\text{ASX}} \\cdot \\Delta S_{\\text{ASX}} \\\\\n",
    "\\Delta V_{\\text{SPX}} & = N_{\\text{SPX}} \\cdot S_{\\text{FX}} \\cdot \\Delta S_{\\text{SPX}} + N_{\\text{SPX}} \\cdot S_{\\text{SPX}} \\cdot \\Delta S_{\\text{FX}} \\\\\n",
    "\\Delta V_{\\text{USD}} & = N_{\\text{USD}} \\cdot \\Delta S_{\\text{FX}} \\\\\n",
    "\\Delta V_{B_1} & = C_{0.5}^{(1)} \\cdot \\Delta P_{0.5} + C_1^{(1)} \\cdot \\Delta P_1\\\\\n",
    "\\Delta V_{B_2} & = C_{0.5}^{(2)} \\cdot \\Delta P_{0.5} + C_1^{(2)} \\cdot \\Delta P_1 + C_2^{(2)} \\cdot \\Delta P_2,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $C_t^{(i)}$ is the cash flow (coupon payment and/or principal, including mapped CFs) for bond $i$ at time $t$, for $i=1,2$ and $t = 0.5,1,2$, and $P_t$ is the price of a zero coupon bond with maturity $t$. Above, we omit the time argument '$(t)$.'\n",
    "\n",
    "Using the approximation $\\Delta \\ln x \\approx \\frac{\\Delta x}{x}$, we can construct a mapping with respect to log-returns as follows:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\Delta V_{\\text{ASX}} & = N_{\\text{ASX}} \\cdot S_{\\text{ASX}}\\cdot \\Delta \\ln S_{\\text{ASX}} \\\\\n",
    "                      & = V_{\\text{ASX}} \\cdot \\Delta \\ln S_{\\text{ASX}} \\\\\n",
    "\\Delta V_{\\text{SPX}} & = N_{\\text{SPX}} \\cdot S_{\\text{FX}} \\cdot S_{\\text{SPX}} \\cdot \\Delta \\ln S_{\\text{SPX}} + N_{\\text{SPX}} \\cdot S_{\\text{FX}} \\cdot S_{\\text{SPX}} \\cdot \\Delta \\ln S_{\\text{FX}} \\\\\n",
    "                      & = V_{\\text{SPX}} \\cdot \\Delta \\ln S_{\\text{SPX}} + V_{\\text{SPX}} \\cdot \\Delta \\ln S_{\\text{FX}} \\\\\n",
    "\\Delta V_{\\text{USD}} & = N_{\\text{USD}} \\cdot S_{\\text{FX}} \\cdot \\Delta \\ln S_{\\text{FX}} \\\\\n",
    "                      & = V_{\\text{USD}} \\cdot \\Delta \\ln S_{\\text{FX}} \\\\\n",
    "\\Delta V_{B_1} & = C_{0.5}^{(1)} \\cdot P_{0.5} \\cdot \\Delta \\ln P_{0.5} + C_1^{(1)} \\cdot P_1 \\cdot \\Delta \\ln P_1\\\\\n",
    "\\Delta V_{B_2} & = C_{0.5}^{(2)} \\cdot P_{0.5} \\cdot \\Delta \\ln P_{0.5} + C_1^{(2)} \\cdot P_1 \\cdot \\Delta \\ln P_1 + C_2^{(2)} \\cdot P_2 \\cdot \\Delta \\ln P_2.\n",
    "\\end{align*}$$\n",
    "\n",
    "(Unfortunately, the change in bond value cannot be simplified in terms of the current value of the bond.)\n",
    "\n",
    "### Covariance matrix of risk factors\n",
    "\n",
    "For convenience, we collect in a single data frame the log-returns of the relevant risk factors. We then calculate the covariance matrix from this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f9a6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASX200</th>\n",
       "      <th>SPXComp</th>\n",
       "      <th>FXrate</th>\n",
       "      <th>AU00Y06</th>\n",
       "      <th>AU01Y00</th>\n",
       "      <th>AU02Y00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASX200</th>\n",
       "      <td>7.531915e-05</td>\n",
       "      <td>1.290643e-05</td>\n",
       "      <td>-6.772445e-06</td>\n",
       "      <td>-3.468750e-08</td>\n",
       "      <td>-1.127068e-07</td>\n",
       "      <td>-1.206142e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPXComp</th>\n",
       "      <td>1.290643e-05</td>\n",
       "      <td>5.885604e-05</td>\n",
       "      <td>-1.631323e-05</td>\n",
       "      <td>2.823914e-09</td>\n",
       "      <td>-1.620656e-07</td>\n",
       "      <td>-9.802465e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FXrate</th>\n",
       "      <td>-6.772445e-06</td>\n",
       "      <td>-1.631323e-05</td>\n",
       "      <td>4.259371e-05</td>\n",
       "      <td>1.105966e-07</td>\n",
       "      <td>4.137853e-07</td>\n",
       "      <td>1.238799e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AU00Y06</th>\n",
       "      <td>-3.468750e-08</td>\n",
       "      <td>2.823914e-09</td>\n",
       "      <td>1.105966e-07</td>\n",
       "      <td>6.544504e-09</td>\n",
       "      <td>1.168089e-08</td>\n",
       "      <td>1.823196e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AU01Y00</th>\n",
       "      <td>-1.127068e-07</td>\n",
       "      <td>-1.620656e-07</td>\n",
       "      <td>4.137853e-07</td>\n",
       "      <td>1.168089e-08</td>\n",
       "      <td>3.423297e-08</td>\n",
       "      <td>9.094777e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AU02Y00</th>\n",
       "      <td>-1.206142e-07</td>\n",
       "      <td>-9.802465e-07</td>\n",
       "      <td>1.238799e-06</td>\n",
       "      <td>1.823196e-08</td>\n",
       "      <td>9.094777e-08</td>\n",
       "      <td>4.036768e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ASX200       SPXComp        FXrate       AU00Y06       AU01Y00  \\\n",
       "ASX200   7.531915e-05  1.290643e-05 -6.772445e-06 -3.468750e-08 -1.127068e-07   \n",
       "SPXComp  1.290643e-05  5.885604e-05 -1.631323e-05  2.823914e-09 -1.620656e-07   \n",
       "FXrate  -6.772445e-06 -1.631323e-05  4.259371e-05  1.105966e-07  4.137853e-07   \n",
       "AU00Y06 -3.468750e-08  2.823914e-09  1.105966e-07  6.544504e-09  1.168089e-08   \n",
       "AU01Y00 -1.127068e-07 -1.620656e-07  4.137853e-07  1.168089e-08  3.423297e-08   \n",
       "AU02Y00 -1.206142e-07 -9.802465e-07  1.238799e-06  1.823196e-08  9.094777e-08   \n",
       "\n",
       "              AU02Y00  \n",
       "ASX200  -1.206142e-07  \n",
       "SPXComp -9.802465e-07  \n",
       "FXrate   1.238799e-06  \n",
       "AU00Y06  1.823196e-08  \n",
       "AU01Y00  9.094777e-08  \n",
       "AU02Y00  4.036768e-07  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the log-returns of all relevant risk factors in a single data frame\n",
    "RF_logret = pd.DataFrame({'ASX200' : fin_data_logret['ASX200'], 'SPXComp' : fin_data_logret['SPXComp'],\n",
    "                         'FXrate' : fin_data_logret['FXrate'], 'AU00Y06' : zcb_logret['AU00Y06'],\n",
    "                         'AU01Y00' : zcb_logret['AU01Y00'], 'AU02Y00' : zcb_logret['AU02Y00']})\n",
    "\n",
    "# Compute the covariance matrix\n",
    "RF_logret_cov = RF_logret.cov()\n",
    "\n",
    "RF_logret_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42db6c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASX200</th>\n",
       "      <th>SPXComp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASX200</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPXComp</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ASX200   SPXComp\n",
       "ASX200   0.000075  0.000013\n",
       "SPXComp  0.000013  0.000059"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_logret_cov.loc[['ASX200', 'SPXComp'], ['ASX200', 'SPXComp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0eb93",
   "metadata": {},
   "source": [
    "## Undiversified VaR\n",
    "\n",
    "To compute the undiversified VaR of the portfolio, we must compute the VaR of each component based on the assumed probability distribution of its respective risk factors. For each instrument/component, we assume that the log-returns of its risk factors $\\mathbf{R}^\\ell_x$ has a (multivariate) normal distribution with mean $\\mathbb{E}(\\mathbf{R}_x^\\ell) = \\mathbf{0}$ and covariance matrix $\\mathbf{\\Sigma} := \\widehat{\\text{Cov}(\\mathbf{R}_x^\\ell)}$, the covariance matrix estimated from the time series of one-day log-returns (**delta-normal approach**). \n",
    "\n",
    "Since the change in the value of each component can be written as $\\Delta V = \\mathbf{W}^\\top \\mathbf{R}_x^\\ell$, where $\\mathbf{W}$ is the vector of (dollar) exposures to each risk factor in $\\mathbf{R}_x^\\ell$, the one-day VaR with confidence level $c\\in(0,1)$ obtained from the delta-normal approach is given by \n",
    "\n",
    "$$\\text{VaR}_c = -\\Phi^{-1}(1-c) \\sqrt{\\mathbf{W}^\\top \\mathbf{\\Sigma} \\mathbf{W}},$$\n",
    "\n",
    "where $\\Phi^{-1}(\\cdot)$ is the inverse cdf of the standard normal distribution.\n",
    "\n",
    "We perform this calculation for each instrument as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb0cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VaR parameter\n",
    "conf_level = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1b0f8",
   "metadata": {},
   "source": [
    "### ASX 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4669f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure to and (co)variance of risk factor(s)\n",
    "ASX_exp = V_ASX \n",
    "ASX_rf_cov = RF_logret_cov.loc['ASX200', 'ASX200']\n",
    "    \n",
    "# One-day VaR\n",
    "ASX_VaR = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt((ASX_exp ** 2) * ASX_rf_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "309bba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92643.5155837443"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPX_VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0bfc4",
   "metadata": {},
   "source": [
    "### S\\&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef97540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure to and (co)variance of risk factor(s)\n",
    "SPX_exp = np.array([V_SPX, V_SPX]) \n",
    "SPX_rf_cov = RF_logret_cov.loc[['SPXComp', 'FXrate'], ['SPXComp', 'FXrate']]\n",
    "\n",
    "# One-day VaR\n",
    "SPX_variance = np.matmul(SPX_exp.transpose(), np.matmul(SPX_rf_cov, SPX_exp))\n",
    "SPX_VaR = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt(SPX_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f380d13",
   "metadata": {},
   "source": [
    "### USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "450c0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure to and (co)variance of risk factor(s)\n",
    "USD_exp = V_USD \n",
    "USD_rf_cov = RF_logret_cov.loc['FXrate', 'FXrate']\n",
    "\n",
    "# One-day VaR\n",
    "USD_VaR = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt((USD_exp ** 2) * USD_rf_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d240c1",
   "metadata": {},
   "source": [
    "### Bond 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a76bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure to and (co)variance of risk factor(s)\n",
    "B1_exp = np.array([B1_coup * zcb_today['AU00Y06'], (B1_fv + B1_coup) * zcb_today['AU01Y00']]) \n",
    "B1_rf_cov = RF_logret_cov.loc[['AU00Y06', 'AU01Y00'], ['AU00Y06', 'AU01Y00']]\n",
    "\n",
    "# One-day VaR\n",
    "B1_variance = np.matmul(B1_exp.transpose(), np.matmul(B1_rf_cov, B1_exp))\n",
    "B1_VaR = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt(B1_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eae574",
   "metadata": {},
   "source": [
    "### Bond 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a3b6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposure to and (co)variance of risk factor(s)\n",
    "B2_exp = np.array([B2_coup * zcb_today['AU00Y06'], (B2_coup + cf_map_to_1y) * zcb_today['AU01Y00'],\n",
    "                  (B2_fv + B2_coup + cf_map_to_2y) * zcb_today['AU02Y00']]) \n",
    "B2_rf_cov = RF_logret_cov.loc[['AU00Y06', 'AU01Y00', 'AU02Y00'], ['AU00Y06', 'AU01Y00', 'AU02Y00']]\n",
    "\n",
    "# One-day VaR\n",
    "B2_variance = np.matmul(B2_exp.transpose(), np.matmul(B2_rf_cov, B2_exp))\n",
    "B2_VaR = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt(B2_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55c1b8",
   "metadata": {},
   "source": [
    "### One-Day VaR\n",
    "\n",
    "The undiversified one-day 90\\% VaR for the portfolio is then the sum of one-day 90\\% VaR of each instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db8ffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205558.22184449038"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR_undiversified = ASX_VaR + SPX_VaR + USD_VaR + B1_VaR + B2_VaR\n",
    "VaR_undiversified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bddf6c",
   "metadata": {},
   "source": [
    "**That is, over the next trading day, there is a 10\\% probability that the loss in portfolio value will exceed AU\\$ 205,558.22.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeac278",
   "metadata": {},
   "source": [
    "### 10-Day VaR\n",
    "\n",
    "The use of log-returns in the risk factor mapping allows us to use square-root-of-time scaling to compute the 10-day VaR. Specifically, we have\n",
    "\n",
    "$$\\text{10-day VaR} = \\sqrt{10} \\cdot \\text{one-day VaR}.$$\n",
    "\n",
    "For this particular portfolio, the undiversified 10-day VaR is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc990292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650032.1728027677"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR_undiversified_10day = np.sqrt(10) * VaR_undiversified\n",
    "VaR_undiversified_10day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84526e0",
   "metadata": {},
   "source": [
    "**Intuitively, over (at the end of) the next 10 trading days, there is a 10\\% probability that the loss in portfolio value will exceed AU\\$ 650,032.17.**\n",
    "\n",
    "**NOTE:** To compute the 10-day VaR without the time scaling technique, we need to use the 10-day returns instead of the one-day returns in $\\mathbf{R}_x^\\ell$. Computing the VaR using the formula above will (naturally) result to the 10-day VaR for the portfolio (or each instrument)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c19d1",
   "metadata": {},
   "source": [
    "## Diversified VaR\n",
    "\n",
    "Computing the diversified VaR for the entire portfolio involves determining the exposure to each relevant risk factor *at the level of the portfolio* and using the full covariance matrix of the log-returns of the risk factors.\n",
    "\n",
    "From the risk factor maps above, we have the following expression\n",
    "\n",
    "$$\\left[\\begin{array}{c}\n",
    "\\Delta V_{\\text{ASX}} \\\\ \\Delta V_{\\text{SPX}} \\\\ \\Delta V_{\\text{USD}} \\\\ \\Delta V_{B_1} \\\\ \\Delta V_{B_2} \n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{cccccc}\n",
    "V_{\\text{ASX}} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & V_{\\text{SPX}} & V_{\\text{SPX}} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & V_{\\text{USD}} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & C_{0.5}^{(1)} P_{0.5} & C_1^{(1)} P_1 & 0 \\\\\n",
    "0 & 0 & 0 & C_{0.5}^{(2)} P_{0.5} & C_1^{(2)} P_1 & C_2^{(2)} P_2\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta \\ln S_{\\text{ASX}} \\\\ \\Delta \\ln S_{\\text{SPX}} \\\\ \\Delta \\ln S_{\\text{FX}} \\\\ \\Delta \\ln P_{0.5} \\\\ \\Delta \\ln P_1 \\\\ \\Delta \\ln P_2\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "At the portfolio level, the change in portfolio value exposed to each of the relevant risk factors in the right-most vector above. The dollar exposure of the portfolio with respect to each risk factor is given by the **column sums of the matrix above.** Thus, we have\n",
    "\n",
    "$$\\Delta V = \\mathbf{W}^\\top \\mathbf{R}_x^\\ell,$$ \n",
    "\n",
    "where $\\Delta V$ is the change in the portfolio value, $\\mathbf{R}_x^\\ell$ is the vector of log-returns of the risk factors, and\n",
    "\n",
    "$$\\mathbf{W} = \\left[\\begin{array}{c}\n",
    "V_{\\text{ASX}} \\\\ V_{\\text{SPX}} \\\\ V_{\\text{SPX}} + V_{\\text{USD}} \\\\ C_{0.5}^{(1)} P_{0.5} + C_{0.5}^{(2)} P_{0.5} \\\\ C_1^{(1)} P_1 + C_1^{(2)} P_1 \\\\ C_2^{(2)} P_2\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "The (diversified) portfolio VaR can then be calculated from the formula above, where $\\mathbf{\\Sigma}$ is the covariance matrix of *all* relevant risk factors. The calculation is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36fca3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136579.27495678866"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Portfolio exposures\n",
    "portfolio_exp = np.array([V_ASX, V_SPX, V_SPX + V_USD, \n",
    "                         B1_coup * zcb_today['AU00Y06'] + B2_coup * zcb_today['AU00Y06'],\n",
    "                         (B1_fv + B1_coup) * zcb_today['AU01Y00'] + (B2_coup + cf_map_to_1y) * zcb_today['AU01Y00'],\n",
    "                         (B2_fv + B2_coup + cf_map_to_2y) * zcb_today['AU02Y00']])\n",
    "\n",
    "# Portfolio variance\n",
    "portfolio_variance = np.matmul(portfolio_exp.transpose(), np.matmul(RF_logret_cov, portfolio_exp))\n",
    "\n",
    "# Diversified one-day VaR\n",
    "VaR_diversified = -sp.stats.norm.ppf(1 - conf_level) * np.sqrt(portfolio_variance)\n",
    "VaR_diversified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74f205",
   "metadata": {},
   "source": [
    "We observe that the diversified VaR AU \\\\$136,579.27 is markedly lower than the undiversified VaR AU \\\\$205,558.13. This reflects any diversification benefits arising from the historical dynamics of the risk factors. That is, it is possible that adverse movements in one risk factor is offset by beneficial movements in another, as captured by the full covariance matrix of the risk factors.\n",
    "\n",
    "The 10-day VaR can likewise be approximated using square-root-of-time scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42f068ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431901.5900378474"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR_diversified_10day = np.sqrt(10) * VaR_diversified\n",
    "VaR_diversified_10day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60120f70",
   "metadata": {},
   "source": [
    "The result is also lower than the scaled undiversified VaR.\n",
    "\n",
    "**NOTE:** The diversified 10-day VaR can also be computed without scaling by using the 10-day returns instead of one-day returns in $\\mathbf{R}_x^\\ell$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
